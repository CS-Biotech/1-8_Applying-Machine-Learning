{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Applying Machine Learning\n",
    "# Additional Exercises\n",
    "\n",
    "In this notebook, you will explore new machine learning techniques. Below we have imported the necessary modules/packages and split the data the same way as in Week 5-7. Run the code below for setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', index_col=0)\n",
    "\n",
    "# Data cleaning\n",
    "# remove the 'Unnamed: 32' column\n",
    "df = df.drop('Unnamed: 32', axis=1)\n",
    "\n",
    "# encode target feature to binary class and split target/predictor vars\n",
    "y = df[\"diagnosis\"].map({\"B\" : 0, \"M\" : 1})\n",
    "X = df.drop(\"diagnosis\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique. It transforms high-dimensional data into a lower-dimensional form while preserving as much variability (or information) as possible. PCA achieves this by identifying the principal components of the data, which are the directions of maximum variance.\n",
    "\n",
    "- **Goal:** Reduce the number of features (dimensions) while retaining the most important information in the data.\n",
    "- **How it works:**\n",
    "  1. **Standardization:** Make sure all features have the same scale (mean 0, variance 1), to ensure no variable dominates.\n",
    "  2. **Calculate Covariance Matrix:** This helps to understand the relationships between the variables.\n",
    "  3. **Compute Eigenvectors and Eigenvalues:** The eigenvectors correspond to the directions of maximum variance, and the eigenvalues indicate how much variance each eigenvector captures.\n",
    "  4. **Sort Eigenvectors:** Sort them by eigenvalue in descending order. The top eigenvectors will become the principal components. Choose which component to keep (low eigenvalue = less significant).\n",
    "  5. **Transform the Data:** Project the data onto the new subspace formed by the principal components.\n",
    "\n",
    "- **Use Cases:**\n",
    "  - Reducing the dimensionality of large datasets to improve performance in downstream tasks.\n",
    "  - Visualizing high-dimensional data (often in 2D or 3D).\n",
    "  - Noise reduction by keeping only the most significant components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA - reduce the data to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Breast Cancer Dataset')\n",
    "plt.colorbar(scatter, label='Diagnosis (0 = Benign, 1 = Malignant)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Means Clustering\n",
    "K-means is an unsupervised machine learning algorithm used for clustering. It groups data into a predefined number of clusters based on similarity.\n",
    "\n",
    "- **Goal:** Partition the data into K distinct clusters such that the data points within each cluster are as similar as possible.\n",
    "- **How it works:**\n",
    "  1. **Initialization:** Randomly select K points as the initial cluster centroids.\n",
    "  2. **Assignment Step:** Assign each data point to the nearest centroid.\n",
    "  3. **Update Step:** Recalculate the centroids by taking the mean of all data points assigned to each cluster.\n",
    "  4. **Repeat:** Continue steps 2 and 3 until the centroids stop changing or a set number of iterations is reached.\n",
    "\n",
    "- **Use Cases:**\n",
    "  - Grouping similar data points for pattern recognition, like customer segmentation, document classification, etc.\n",
    "  - Image compression and anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is used to discover natural groupings or structure in the data when you don't have predefined labels. However, it's a good method to test the dimensionality reduction technique. Let's try running K-means on our PCA data. We will cluster with 2 centroids since the target feature (y) has 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering (we use K=2 because there are two classes: Benign and Malignant)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Get the predicted cluster labels\n",
    "y_kmeans = kmeans.labels_\n",
    "\n",
    "# Visualize the KMeans clusters\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('K-Means Clustering on PCA-Reduced Breast Cancer Data')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the cluster centers\n",
    "print(\"Cluster Centers (in PCA-reduced space):\")\n",
    "print(kmeans.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results of PCA with the true labels and compare them side-by-side with the KMeans clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster centroids from previously fitted KMeans\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Plot 1: KMeans Clusters ---\n",
    "scatter1 = axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7, edgecolor='k')\n",
    "axs[0].scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "axs[0].set_title('KMeans Clustering (PCA Space)', fontsize=14)\n",
    "axs[0].set_xlabel('Principal Component 1')\n",
    "axs[0].set_ylabel('Principal Component 2')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# --- Plot 2: True Labels ---\n",
    "scatter2 = axs[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolor='k')\n",
    "axs[1].set_title('True Labels (PCA Space)', fontsize=14)\n",
    "axs[1].set_xlabel('Principal Component 1')\n",
    "axs[1].set_ylabel('Principal Component 2')\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Layout adjustment\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we can see that the K-means clusters closely resemble the true labels, indicating that PCA successfully preserved the key structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q*1: Run the PCA and K-means algorithm on the wine dataset (refer to [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html)) data, and plot your results.** \n",
    "\n",
    "> Hint: Consider the values you set for the centroids\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "\n",
    "\n",
    "# Apply PCA - reduce the data to 2 components\n",
    "\n",
    "\n",
    "# Visualize the PCA result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering (we use K=3)\n",
    "\n",
    "\n",
    "# Get the predicted cluster labels\n",
    "\n",
    "\n",
    "# Visualize the K-Means clusters\n",
    "\n",
    "\n",
    "# Print the cluster centers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this module, we explored two fundamental techniques in unsupervised learning: Principal Component Analysis (PCA) for dimensionality reduction and K-Means Clustering for grouping similar data points. PCA allowed us to reduce the complexity of high-dimensional data while retaining most of its variance, making patterns more interpretable and computationally manageable. K-Means, on the other hand, enabled us to discover inherent structures within the data by assigning observations to clusters based on similarity.\n",
    "\n",
    "Together, these methods demonstrate a powerful workflow: using PCA to simplify data before applying K-Means can enhance clustering performance and visualization. Understanding and combining these techniques is a valuable step in making sense of complex datasets, forming the foundation for more advanced data analysis and machine learning tasks. In HMB301, we will explore further techniques of dimensional reduction and clustering. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
